---
date created: 2022-04-08 10:21
---

# Data Wrangling

To wrangle(整理) data, we need two things: data to wrangle, and something to do with it. Logs often make for a good use-caes.

server's log:

```shell
ssh myserver journalctl
```

That's far to much stuff. Let's limit it to ssh stuff:

```shell
ssh myserver journalctl | grep sshd
```

Using a pite to stream a remote file through grep on our local computer. This still way more stuff than we wanted though. And pretty hard to read. Do better:

```shell
ssh myserver 'journalctl | grep ssh | grep "Disconnected from"' | less
```

**Why the additional quoting**? 因为logs文件可能会很大，如果通过文件流把全部传输到本地电脑将是很浪费的。与之相反，我们可以先在远端进行数据筛选然后再把数据传输到本地。`less` 给我们提供了一个页面可以滚动浏览。
也可以将数据保存到文件，供之后进行筛选，而不是每次都通过网络传输。 ^c4a81a

```shell
$ ssh myserver 'journalctl | grep sshd | grep "Disconnected "' > ssh.log
$ less ssh.log
```

仍有许多不需要的信息。有很多方法解决， 这里我们学习最强大的工具:  `sed`.

`sed` 是一个 "stream editor" 流编辑器 基于 old `ed` editor. 在其中，您只用基本上给出了有关如何修改文件的简短命令，而不是直接操作其内容（尽管您也可以这样做）。One of the most common ones is `s`: substitution. ^325387

```shell
ssh myserver journalctl
	| grep sshd
	| grep "Disconnected from"
	| sed 's/.*Disconnected from //'
```

we just wrote was a simple regular expression. The `s` command is written on the form: `s/REGEX/SUBSTITUTION/`.

## Regular expressions

`/.*Disconnected from /`. Regular expression are usually (though not always) surrounded by `/`. Very common patterns are:

- `.` means "any single character" except newline
- `*`  **zero** or more of the precding match
- `+`  **one** or more of the precding match
- `[abc]` any one character of `a`, `b`, and `c`
- `(RX1|RX2)` either something that matches `RX1` or `RX2`
- `^` the start of the line
- `$` the end of the line

`sed`'s regular expression are somewath weird, and will require you to put `\` before most of these to give them their special meaning. Or you can pass `-E`

beware(谨防), regular expression are trixy(麻烦的). 如果我们在日志里遇到以 `Disconnected from` 用户登录的，我们该怎么做:

```shell
Jan 17 03:13:00 thesquareplanet.com sshd[2631]: Disconnected from invalid user Disconnected from 46.97.239.16 port 55920 [preauth]
```

`*`, `+` 默认是贪婪的。它们会匹配尽可能多的字符。因此它会匹配直到最后一个`Disconnected from` 所以上面的例子中, 我们会得到这样的一行

```shell
46.97.239.16 port 55920 [preauth]
```

在一些正则表达式实现中， 我们可以给 `*`, `+` suffix(后缀) 上一个 `?` 去使之不要去贪婪匹配. 但是遗憾的是 `sed` 并不支持。 可以切换到 `perl` 的命令行模式来解决， 它是支持的: ^79c4e3

```shell
perl -pe 's/.*?Disconnected from //'
```

> 有关 perl 命令行用法文档
> man perldoc
> man perlrun
> -p: Places a printing loop around your command so that it acts on each line of standard input.，在您的命令周围放置一个打印循环，以便它作用于每个 标准输入行。有点像 sed
> -e commandline 进入单行程序
> Allows you to provide the program as an argument rather than in a file. You don't want to have to create a script file for every little Perl one-liner.
>
> # [Perl flags -pe, -pi, -p, -w, -d, -i, -t?](https://stackoverflow.com/questions/6302025/perl-flags-pe-pi-p-w-d-i-t)

^4b309a
重新回到 `sed`, 因为它更接近日常使用。

好的，所以我们还有一个想要去掉的后缀。 我们可以怎么做 那？ 仅匹配后面的文本有点棘手 用户名，特别是如果用户名可以有空格等！ 我们做什么 需要做的是匹配 _整_ 行：

```shell
| sed -E 's/.*Disconnected from (invalid |authenticating )?user .* [^ ]+ port [0-9]+( \[preauth\])?$//'
```

[正则表达式 调试器](https://regex101.com/r/qqbZqh/2)
然后我们匹配任何字符串 用户名所在的字符。 然后我们匹配任何一个单词 ( `[^ ]+`; 任何非空的非空格字符序列）。 然后这个词 “端口”后跟一系列数字。 然后可能是后缀 `[preauth]`，然后是行尾。
这样匹配会使全部都匹配上的行都被替换空行，所以我们看到的只有空行。因此我们使用捕获组(Capture groups). 任何被正则匹配的文本带有括号包围的会被储存在一组由数字标号的捕获组中。这些是可用的 在替换中（在某些引擎中，甚至在模式本身中！） 作为 `\1`, `\2`, `\3`等。所以：

```shell
| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
```

众所周知，正则表达式很难正确，但它们也是 在您的工具箱中非常方便！

## Back to data wrangling

````shell
ssh myserver journalctl
	| grep sshd
	| grep "Disconnected from"
	| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
	```
`sed` 也可以做插入文本(`i` 命令)， 显示打印行(`p` 命令), 按索引选择行
```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
````

`sort` 对输入进行排序。 `uniq -c` 将连续且相同的行合并为一行， 前缀为出现次数. ^9707b3

对其排序，只保留最常见的用户名

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
```

`sort -n` 将按数字排序(而不是字典). `-k1,1` 表示仅按第一个空格分隔的列排序
`,n` 表示只对 `n` 前的域排序， 默认排序直到结束. 对于现在这个特定例子即使整行参与排序也没有影响。 ^01ae7a

如果只想保留最不常见的用户名，可以使用 `head` 代替 `tail`. 或者使用 `sort -r` 逆序排序。

如果我们不想要用户名是每行一个输出，而是想要用逗号分隔并合并在一行输出.

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | awk '{print $2}' | paste -sd,
```

这里使用 `awk` 而不是 `cut -d ' ' -f N`, 因为我发现 cut 不够智能， 它仍然把分割出的空格也作为一个域，因此不能对齐输出

`paste`: it lets you combine lines (`-s`) by a given single-character delimiter (`-d`; `,` in this case).
^e78405

> -s 同时粘贴而不是一次粘贴一个
> -d 分割符

## awk - another editor

`awk` 是一种编程语言但恰巧十分适合用来处理文本流。 ^7864a6

`{print $2}` `awk` 程序采用一个模式模式串（可选）加上一个代码块，来说明该模式该怎么匹配给定的行。默认匹配所有行,  `$0` 为整行内容, `$1` 到 `$n` 为一行中的 n 个区域，区域的分割根据 `awk` 的域分隔符（默认为空格，可以通过 `-F` 来修改）。

统计所有以 `c` 开头, 以 `e` 结尾，且只尝试登录过一次的用户。

```shell
 | awk '$1 == 1 && $2 ~ /^c[^ ]*e$/ { print $2 }' | wc -l
```

为 `awk`指定了一个匹配模式串（也就是`{...}`前面的那部分内容）。该匹配要求文本的第一部分需要等于1（这部分刚好是`uniq -c`得到的计数值），然后其第二部分必须满足给定的一个正则表达式。代码块中的内容则表示打印用户名。然后我们使用 `wc -l` 统计输出结果的行数。

不过，既然 `awk` 是一种编程语言，那么则可以这样：

```shell
BEGIN { rows = 0 }
$1 == 1 && $2 ~ /^c[^ ]*e$/ { rows += $1 }
END { print rows }
```

`BEGIN` 也是一种模式，它会匹配输入的开头（ `END` 则匹配结尾）。然后，对每一行第一个部分进行累加，最后将结果输出。

## 分析数据

想做数学计算也是可以的！例如这样，您可以将每行的数字加起来：

```shell
 | paste -sd+ | bc -l
```

下面这种更加复杂的表达式也可以：

```shell
echo "2*($(data | paste -sd+))" | bc -l
```

您可以通过多种方式获取统计数据。如果已经安装了R语言，[`st`](https://github.com/nferraz/st)是个不错的选择：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | awk '{print $1}' | R --slave -e 'x <- scan(file="stdin", quiet=TRUE); summary(x)'
```

R 也是一种编程语言，它非常适合被用来进行数据分析和[绘制图表](https://ggplot2.tidyverse.org/)。这里我们不会讲的特别详细， 您只需要知道`summary` 可以打印某个向量的统计结果。我们将输入的一系列数据存放在一个向量后，利用R语言就可以得到我们想要的统计数据。 ^07e324

如果您希望绘制一些简单的图表， `gnuplot` 可以帮助到您：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | gnuplot -p -e 'set boxwidth 0.5; plot "-" using 1:xtic(2) with boxes'
```

## 利用数据整理来确定参数

有时候您要利用数据整理技术从一长串列表里找出你所需要安装或移除的东西。我们之前讨论的相关技术配合 `xargs` 即可实现：

```shell
rustup toolchain list | grep nightly | grep -vE "nightly-x86" | sed 's/-x86.*//' | xargs rustup toolchain uninstall
```

## 整理二进制数据

虽然到目前为止我们的讨论都是基于文本数据，但对于二进制文件其实同样有用。例如我们可以用 ffmpeg 从相机中捕获一张图片，将其转换成灰度图后通过SSH将压缩后的文件发送到远端服务器，并在那里解压、存档并显示。

```shell
ffmpeg -loglevel panic -i /dev/video0 -frames 1 -f image2 -
 | convert - -colorspace gray -
 | gzip
 | ssh mymachine 'gzip -d | tee copy.jpg | env DISPLAY=:0 feh -'
```

`-` 的含义由程序决定，不是关键字
普遍约定是表示标准输入，但是程序也可以选择其他意思
cd的文档会写 `cd -` 表示进入上次所在目录
ffmpeg `-` is the same as `pipe:`

`-` in other Linux utilities as mentioned [in the documentation of the pipe protocol](https://ffmpeg.org/ffmpeg-protocols.html#pipe):

> If number is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.

`-`在输出端意味着 `stdout`. 你也可以写 `pipe:1`在它的位置。 作为输入，这意味着 `stdin`并且可以写成 `pipe:0`.

> 相关： [什么是“破折号” - 表示 ffmpeg 输出文件名](https://stackoverflow.com/questions/63596219/what-does-dash-mean-as-ffmpeg-output-filename)

^873101

![[Pasted image 20220405160855.png]]

## Exercise

- 统计words文件 (`/usr/share/dict/words`) 中包含至少三个`a` 且不以`'s` 结尾的单词个数。这些单词中，出现频率前三的末尾两个字母是什么？ `sed`的 `y`命令，或者 `tr` 程序也许可以帮你解决大小写的问题。共存在多少种词尾两字母组合？还有一个很 有挑战性的问题：哪个组合从未出现过？

大写字符转换为小写 ^ef3b58

1. `sed 'y/ABCDEFGHIJKLMNOPQRSTUVWXYZ/abcdefghijklmnopqrstuvwxyz/'`
2. `tr "[:upper:]" "[:lower:]"`
3. `awk '{print tolower($0)}'`

**Solution**:

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | wc -l
```

- 非`a`字符加一个`a` 组重复三次 {3}
- 取反

出现频率前三的末尾两个字母是什么？

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq -c | sort | tail -n3
```

共存在多少种词尾两字母组合？

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq | wc -l
```

哪个组合从未出现过？ 为了得到没出现的组合，首先我们要生成一个包含全部组合的列表，然后再使用上面得到的出现的组合，比较二者不同即可。

```shell
#!/bin/bash
for i in {a..z};do
 for j in {a..z};do
    echo  "$i$j"
 done
done
```

```shell
./all.sh > all.txt
```

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq > occurance.txt
```

`--unchanged-group-format=''`用于将两个文件中相同的内容设置为空字符串，剩下的内容就是差异的部分。

```shell
diff --unchanged-group-format='' <(cat occurance.txt) <(cat all.txt) | wc -l
```

2. 进行原地替换听上去很有诱惑力，例如： `sed s/REGEX/SUBSTITUTION/ input.txt > input.txt`。但是这并不是一个明智的做法，为什么呢？还是说只有 `sed`是这样的? 查看 `man sed` 来完成这个问题

`sed s/REGEX/SUBSTITUTION/ input.txt > input.txt` 表达式中后一个 `input.txt`会首先被清空，而且是发生在前的。所以前面一个`input.txt`在还没有被 `sed` 处理时已经为空了。在使用正则处理文件前最好是首先备份文件。 ^dbaffb

```shell
sed -i.bak s/REGEX/SUBSTITUTION/ input.txt > input.txt
```

可以自动创建一个后缀为 `.bak` 的备份文件。
3. 找出您最近十次开机的开机时间平均数、中位数和最长时间。在Linux上需要用到 `journalctl` ，而在 macOS 上使用 `log show`。找到每次起到开始和结束时的时间戳。在Linux上类似这样操作：

```
Logs begin at ...
```

和

```
systemd[577]: Startup finished in ...
```

在 macOS 上, [查找](https://eclecticlight.co/2018/03/21/macos-unified-log-3-finding-your-way/):

```
=== system boot:
```

和

```
Previous shutdown cause: 5
```

为了进行这个练习，我们需要首先允许`journalctl`记录多次开机的日志，具体背景信息可以参考[这里](https://www.digitalocean.com/community/tutorials/how-to-use-journalctl-to-view-and-manipulate-systemd-logs)和[这里](https://askubuntu.com/questions/765315/how-to-find-previous-boot-log-after-ubuntu-16-04-restarts)否则我们看到的始终都只有本次启动的日志。

```
vim /etc/systemd/journald.conf
```

设置`Storage=persistent` 执行上述命令后，重启

```
pi@raspberrypi:~$ journalctl --list-boots
-1 d176984f171a4ceba353de47abd2b891 Thu 2021-05-27 15:55:36 BST—Fri 2021-05-28 02:09:50 BST
0 18c4819a536548a29def9f2b56f63dd0 Fri 2021-05-28 02:09:51 BST—Fri 2021-05-28 02:25:50 BST
```

可以看到已经可以列出多次启动信息了，然后我们进行十次重启。 ![1.png](https://missing-semester-cn.github.io/missing-notes-and-solutions/2020/solutions/images/4/2.png)
可以使用 `systemd-analyze`工具看一下启动时间都花在哪里：

```
sudo systemd-analyze plot > systemd.svg
```

![1.png](https://missing-semester-cn.github.io/missing-notes-and-solutions/2020/solutions/images/4/3.svg)\
_右键图片新窗口打开查看大图_\
可以看到启动时间为 14.157s。 接下来，编写脚本`getlog.sh`来获取最近十次的启动时间数据：

```shell
#!/bin/bash
for i in {0..9}; do
   journalctl -b$i | grep "Startup finished in"
done
```

```shell
./getlog > starttime.txt
```

```shell
#获取最长时间
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort | tail -n1
#获取最短时间
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort -r | tail -n1
#平均数（注意 awk 要使用单引号）
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| paste -sd+ | bc -l | awk '{print $1/10}'
# 中位数
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort |paste -sd\  | awk '{print ($5+$6)/2}'
```

如果配合使用 R 语言脚本则更加简单：

```shell
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort | R -e 'd<-scan("stdin",quiet=TRUE);min(d);max(d);mean(d);median(d);'
```

```shell
> d<-scan("stdin",quiet=TRUE);min(d);max(d);mean(d);median(d);
[1] 14.023
[1] 15.989
[1] 14.4304
[1] 14.2915
```

4. 查看之前三次重启启动信息中不同的部分(参见 `journalctl`的`-b` 选项)。将这一任务分为几个步骤，首先获取之前三次启动的启动日志，也许获取启动日志的命令就有合适的选项可以帮助您提取前三次启动的日志，亦或者您可以使用`sed '0,/STRING/d'` 来删除`STRING`匹配到的字符串前面的全部内容。然后，过滤掉每次都不相同的部分，例如时间戳。下一步，重复记录输入行并对其计数(可以使用`uniq` )。最后，删除所有出现过3次的内容（因为这些内容上三次启动日志中的重复部分）。 简单修改上面使用的`getlog.sh`，获取最近三次的日志，然后使用下面的命令：

```shell
#注意 uniq 只能过滤相邻的行，所以必须先排序
cat last3start.txt | sed -E "s/.*pi\ (.*)/\1/" | sort | uniq -c | sort | awk '$1!=3  { print }'
```
